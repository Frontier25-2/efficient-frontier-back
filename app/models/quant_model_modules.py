import pandas as pd
import numpy as np

from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns
from scipy.optimize import minimize


# ---------- Helper ----------
def _weights_dict_to_array(wdict, columns):
    return np.array([wdict.get(col, 0.0) for col in columns])


def clean_weights_array(arr, cutoff=1e-8, rounding=6):
    a = np.array(arr).copy()
    a[np.abs(a) < cutoff] = 0.0
    if rounding is not None:
        a = np.round(a, rounding)
    s = a.sum()
    if s != 0:
        a = a / s
    else:
        a = np.ones_like(a) / len(a)
    return a


def _clean_returns_df(returns_df: pd.DataFrame) -> pd.DataFrame:
    df = returns_df.replace([np.inf, -np.inf], np.nan).dropna(how="any")
    return df


# ---------- ê³µí†µ: í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ ê³„ì‚° ----------
def _compute_portfolio_stats(returns: pd.DataFrame, weights, annual_freq: int = 252):
    """
    returns: (T x N) ìˆ˜ìµë¥ 
    weights: ê¸¸ì´ N ë²¡í„°
    """
    w = np.array(weights, dtype=float)

    if returns.empty:
        return float("nan"), float("nan")

    port_ret = returns.values @ w

    mean_daily = np.nanmean(port_ret)
    std_daily = np.nanstd(port_ret, ddof=1)

    expected_return = float(mean_daily * annual_freq)
    risk = float(std_daily * np.sqrt(annual_freq))

    return risk, expected_return


def _get_recent_window(returns: pd.DataFrame, last_index, lookback: int | None):
    """
    ë§ˆì§€ë§‰ rebalancing ì‹œì (last_index)ê¹Œì§€ì˜ êµ¬ê°„ ì¤‘ ìµœê·¼ lookback ê¸°ê°„ë§Œ ìž˜ë¼ì„œ ë¦¬í„´
    """
    try:
        sub = returns.loc[:last_index]
    except Exception:
        sub = returns

    if lookback is not None and len(sub) > lookback:
        sub = sub.iloc[-lookback:]

    return sub


# ---------- utility: build rebalancing indices ----------
def _build_rebal_index(
    returns_index: pd.DatetimeIndex, lookback_periods: int, rebal_periods: str
):
    if len(returns_index) <= lookback_periods:
        return returns_index

    start = returns_index[lookback_periods]
    end = returns_index[-1]

    cand = pd.date_range(start, end, freq=rebal_periods)
    rebal_indice = returns_index[returns_index.get_indexer(cand, method="ffill")]

    rebal_indice = rebal_indice.unique()
    rebal_indice = rebal_indice[rebal_indice.notnull()]
    return rebal_indice


# ---------- Maximize Diversification ----------
def MaximizeDiversification(
    rebal_periods: str,
    returns: pd.DataFrame,
    lookback_periods: int,
    bnd=None,
    long_only=True,
    frequency: int = 252,
):
    returns = _clean_returns_df(returns)
    cols = returns.columns

    def calc_diversification_ratio(w, V):
        w = np.array(w)
        w_vol = np.dot(np.sqrt(np.diag(V)), w.T)
        port_vol = np.sqrt(np.dot(w.T, np.dot(V, w)))
        diversification_ratio = w_vol / port_vol if port_vol > 0 else 0.0
        return -diversification_ratio

    def total_weight_constraint(x):
        return x.sum() - 1.0

    def get_weights(w0, V, bnd, long_only):
        cons = ({"type": "eq", "fun": total_weight_constraint},)
        if long_only and bnd is None:
            bnd = [(0, 1)] * len(w0)
        try:
            res = minimize(
                calc_diversification_ratio,
                w0,
                bounds=bnd,
                args=(V,),
                method="SLSQP",
                constraints=cons,
            )
            W = clean_weights_array(res.x)
        except Exception:
            W = np.ones(len(w0)) / len(w0)
        return W

    w0 = np.array([1 / len(cols)] * len(cols))
    rebal_indice = _build_rebal_index(returns.index, lookback_periods, rebal_periods)

    Pw_list = []
    for idx in rebal_indice:
        ext_returns = returns.loc[:idx].iloc[-lookback_periods:]
        try:
            V = risk_models.sample_cov(
                ext_returns, returns_data=True, frequency=frequency
            ).values
        except Exception:
            V = ext_returns.cov().values

        W = get_weights(w0, V, bnd, long_only)
        Pw_list.append(W)

    Pw = pd.DataFrame(Pw_list, index=rebal_indice, columns=cols)
    return Pw


# ---------- Mean-Variance Max Sharpe ----------
def MeanVarianceMaxSharpe(
    rebal_periods: str,
    returns: pd.DataFrame,
    lookback_periods: int,
    frequency: int = 252,
):
    returns = _clean_returns_df(returns)
    cols = returns.columns

    rebal_indice = _build_rebal_index(returns.index, lookback_periods, rebal_periods)
    Pw_list = []

    for idx in rebal_indice:
        ext_df = returns.loc[:idx].iloc[-lookback_periods:]

        if ext_df.shape[0] < max(10, len(cols)):
            W = np.ones(len(cols)) / len(cols)
            Pw_list.append(W)
            continue

        try:
            mu = expected_returns.mean_historical_return(
                ext_df, returns_data=True, compounding=True, frequency=frequency
            )
            S = risk_models.sample_cov(ext_df, returns_data=True, frequency=frequency)
            ef = EfficientFrontier(mu, S)
            ef.max_sharpe()
            wdict = ef.clean_weights()
            W = clean_weights_array(_weights_dict_to_array(wdict, cols))
        except Exception:
            W = np.ones(len(cols)) / len(cols)

        Pw_list.append(W)

    Pw = pd.DataFrame(Pw_list, index=rebal_indice, columns=cols)
    return Pw


# ---------- Mean-Variance Min Volatility ----------
def MeanVarianceMinVolatility(
    rebal_periods: str,
    returns: pd.DataFrame,
    lookback_periods: int,
    frequency: int = 252,
):
    returns = _clean_returns_df(returns)
    cols = returns.columns

    rebal_indice = _build_rebal_index(returns.index, lookback_periods, rebal_periods)
    Pw_list = []

    for idx in rebal_indice:
        ext_df = returns.loc[:idx].iloc[-lookback_periods:]

        if ext_df.shape[0] < max(10, len(cols)):
            W = np.ones(len(cols)) / len(cols)
            Pw_list.append(W)
            continue

        try:
            mu = expected_returns.mean_historical_return(
                ext_df, returns_data=True, compounding=True, frequency=frequency
            )
            S = risk_models.sample_cov(ext_df, returns_data=True, frequency=frequency)
            ef = EfficientFrontier(mu, S)
            ef.min_volatility()
            wdict = ef.clean_weights()
            W = clean_weights_array(_weights_dict_to_array(wdict, cols))
        except Exception:
            W = np.ones(len(cols)) / len(cols)

        Pw_list.append(W)

    Pw = pd.DataFrame(Pw_list, index=rebal_indice, columns=cols)
    return Pw


# ---------- Risk Parity ----------
def RP(
    rebal_periods: str,
    returns: pd.DataFrame,
    lookback_periods: int,
    frequency: int = 252,
    cov_type: str = "simple",
):
    from pypfopt.risk_models import exp_cov

    returns = _clean_returns_df(returns)
    cols = returns.columns

    def weight_sum_constraint(x):
        return x.sum() - 1.0

    def get_covmat(rets, cov_type):
        if cov_type == "simple":
            return rets.cov().values
        elif cov_type == "exponential":
            return exp_cov(
                rets, returns_data=True, span=len(rets), frequency=frequency
            ).values
        else:
            return rets.cov().values

    def risk_parity_objective(x, covmat):
        x = np.array(x)
        port_var = float(x.T @ covmat @ x)
        if port_var <= 0:
            return 1e9
        sigma = np.sqrt(port_var)
        mrc = (covmat @ x) / sigma
        rc = x * mrc
        return np.sum((rc - rc.mean()) ** 2)

    rebal_indice = _build_rebal_index(returns.index, lookback_periods, rebal_periods)
    Pw_list = []

    for idx in rebal_indice:
        ext_df = returns.loc[:idx].iloc[-lookback_periods:]

        if ext_df.shape[0] < max(10, len(cols)):
            Pw_list.append(np.ones(len(cols)) / len(cols))
            continue

        covmat = get_covmat(ext_df, cov_type)

        x0 = np.repeat(1 / covmat.shape[1], covmat.shape[1])
        cons = ({"type": "eq", "fun": weight_sum_constraint},)

        try:
            res = minimize(
                fun=risk_parity_objective,
                x0=x0,
                args=(covmat,),
                method="SLSQP",
                constraints=cons,
            )
            if res.success:
                W = clean_weights_array(res.x)
            else:
                W = np.ones(len(cols)) / len(cols)
        except Exception:
            W = np.ones(len(cols)) / len(cols)

        Pw_list.append(W)

    Pw = pd.DataFrame(Pw_list, index=rebal_indice, columns=cols)
    return Pw


# ---------- Friendly wrapper functions ----------
def compute_min_variance(
    returns: pd.DataFrame,
    lookback: int = 60,
    freq: str = "M",
    annual_freq: int = 252,
):
    Pw = MeanVarianceMinVolatility(freq, returns, lookback, frequency=annual_freq)
    last_idx = Pw.index[-1]
    w = Pw.loc[last_idx].values

    window = _get_recent_window(returns, last_idx, lookback)
    risk, exp_ret = _compute_portfolio_stats(window, w, annual_freq)

    return w, risk, exp_ret


def compute_max_sharpe(
    returns: pd.DataFrame,
    lookback: int = 60,
    freq: str = "M",
    annual_freq: int = 252,
):
    Pw = MeanVarianceMaxSharpe(freq, returns, lookback, frequency=annual_freq)
    last_idx = Pw.index[-1]
    w = Pw.loc[last_idx].values

    window = _get_recent_window(returns, last_idx, lookback)
    risk, exp_ret = _compute_portfolio_stats(window, w, annual_freq)

    return w, risk, exp_ret


def compute_risk_parity(df: pd.DataFrame, annual_freq: int = 252):
    """
    ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹° í¬íŠ¸í´ë¦¬ì˜¤ ê³„ì‚°
    - weights, risk(ì—°ê°„ ë³€ë™ì„±), expected_return(ì—°ê°„ ê¸°ëŒ€ìˆ˜ìµë¥ ) ë°˜í™˜
    """

    if isinstance(df, pd.Series):
        df = df.to_frame()

    returns = _clean_returns_df(df)
    returns = returns.replace([np.inf, -np.inf], np.nan).dropna(how="any")

    if returns.empty or returns.shape[0] < 5 or returns.shape[1] == 0:
        n = returns.shape[1] if returns.shape[1] > 0 else 0
        if n == 0:
            return np.array([]), float("nan"), float("nan")
        w = np.ones(n) / n
        risk, exp_ret = _compute_portfolio_stats(returns, w, annual_freq)
        return w, risk, exp_ret

    X = returns.values
    n = X.shape[1]

    mu_daily = np.nanmean(X, axis=0)
    mu_daily = np.nan_to_num(mu_daily, nan=0.0, posinf=0.0, neginf=0.0)
    mu = mu_daily * annual_freq

    cov_daily = np.cov(X, rowvar=False)
    cov_daily = np.nan_to_num(cov_daily, nan=0.0, posinf=0.0, neginf=0.0)
    cov_daily = (cov_daily + cov_daily.T) / 2.0
    cov = cov_daily * annual_freq

    if not np.any(cov):
        w = np.ones(n) / n
        risk, exp_ret = _compute_portfolio_stats(returns, w, annual_freq)
        return w, risk, exp_ret

    w0 = np.ones(n) / n

    def risk_contribution(w: np.ndarray, covmat: np.ndarray):
        w = np.asarray(w, dtype=float)
        w = np.clip(w, 0.0, 1.0)
        s = w.sum()
        if s <= 0:
            w = np.ones_like(w) / len(w)
        else:
            w /= s

        port_var = float(w @ covmat @ w)
        if port_var <= 0 or not np.isfinite(port_var):
            return np.zeros_like(w)

        sigma_p = np.sqrt(port_var)
        marginal = covmat @ w
        RC = w * marginal / sigma_p
        RC = np.nan_to_num(RC, nan=0.0, posinf=0.0, neginf=0.0)
        return RC

    def objective(w: np.ndarray, covmat: np.ndarray):
        RC = risk_contribution(w, covmat)
        if not np.all(np.isfinite(RC)):
            return 1e9
        target = RC.mean()
        return float(np.sum((RC - target) ** 2))

    cons = ({"type": "eq", "fun": lambda w: np.sum(np.clip(w, 0.0, 1.0)) - 1.0},)
    bounds = [(0.0, 1.0)] * n

    try:
        res = minimize(
            objective,
            w0,
            args=(cov,),
            method="SLSQP",
            bounds=bounds,
            constraints=cons,
            options={"maxiter": 500},
        )

        if (not res.success) or (not np.all(np.isfinite(res.x))):
            w = w0
        else:
            w = res.x
    except Exception:
        w = w0

    w = clean_weights_array(w)

    risk, exp_ret = _compute_portfolio_stats(returns, w, annual_freq)

    return w, risk, exp_ret


def compute_equal_weight(returns: pd.DataFrame):
    cols = returns.columns
    w = np.ones(len(cols)) / len(cols)
    idx = [returns.index[-1]]
    return pd.DataFrame([w], index=idx, columns=cols)


def compute_max_diversification(
    returns: pd.DataFrame,
    lookback: int = 60,
    freq: str = "M",
    annual_freq: int = 252,
):
    Pw = MaximizeDiversification(freq, returns, lookback, frequency=annual_freq)
    last_idx = Pw.index[-1]
    w = Pw.loc[last_idx].values

    window = _get_recent_window(returns, last_idx, lookback)
    risk, exp_ret = _compute_portfolio_stats(window, w, annual_freq)

    return w, risk, exp_ret


# ---------- ðŸ”¥ Target Risk (Slider 0~1 â†’ ì‹¤ì œ ë¦¬ìŠ¤í¬ ë²”ìœ„ë¡œ ë§¤í•‘) ----------
def compute_target_risk(
    returns: pd.DataFrame,
    target_ratio: float,
    annual_freq: int = 252,
):
    """
    target_ratio: 0~1 ìŠ¬ë¼ì´ë” ìœ„ì¹˜
    """
    if isinstance(returns, np.ndarray): 
            # ìž„ì‹œ ì»¬ëŸ¼ëª… ë¶€ì—¬ (frontier.pyì—ì„œ ìˆ˜ì •í–ˆë‹¤ë©´ ì´ ì¡°ê±´ë¬¸ì—” ì•ˆ ê±¸ë¦¼)
        returns = pd.DataFrame(returns)
        
    returns = _clean_returns_df(returns)
    if returns.empty or returns.shape[1] == 0:
        return np.array([]), float("nan"), float("nan")

    X = returns.values
    n = X.shape[1]

    mu_daily = np.nanmean(X, axis=0)
    mu = mu_daily * annual_freq
    cov_daily = np.cov(X, rowvar=False)
    cov = cov_daily * annual_freq

    if not np.all(np.isfinite(cov)):
        cov = np.nan_to_num(cov)

    w0 = np.ones(n) / n

    diag = np.clip(np.diag(cov), 1e-12, None)
    asset_vols = np.sqrt(diag)
    eq_vol = float(np.sqrt(w0 @ cov @ w0))

    min_risk = float(min(asset_vols.min(), eq_vol))
    max_risk = float(asset_vols.max())

    if not np.isfinite(min_risk) or not np.isfinite(max_risk) or max_risk <= 0:
        port_ret = float(mu @ w0)
        port_vol = float(eq_vol)
        return w0, port_vol, port_ret

    if min_risk >= max_risk:
        max_risk = min_risk * 1.5

    tr = float(target_ratio)
    if not np.isfinite(tr):
        tr = 0.5
    tr = max(0.0, min(1.0, tr))

    target_vol = min_risk + tr * (max_risk - min_risk)

    def objective(w, mu, cov, t_vol, lam=1000.0):
        w = np.array(w)
        w = np.clip(w, 0.0, 1.0)
        s = w.sum()
        if s <= 0:
            w = np.ones_like(w) / len(w)
        else:
            w = w / s

        port_ret = float(w @ mu)
        port_var = float(w @ cov @ w)
        if port_var < 0:
            port_var = 0.0
        vol = np.sqrt(port_var)
        return -port_ret + lam * (vol - t_vol) ** 2

    cons = ({"type": "eq", "fun": lambda w: np.sum(w) - 1.0},)
    bounds = [(0.0, 1.0)] * n

    try:
        res = minimize(
            objective,
            w0,
            args=(mu, cov, target_vol),
            method="SLSQP",
            bounds=bounds,
            constraints=cons,
            options={"maxiter": 500},
        )

        if not res.success:
            w = w0
        else:
            w = res.x

    except Exception as e:
        # [ìˆ˜ì •] ì—ëŸ¬ ë‚´ìš©ì„ ì¶œë ¥í•˜ì—¬ ì›ì¸ íŒŒì•… (ìš´ì˜ í™˜ê²½ì—ì„œëŠ” logging ì‚¬ìš© ê¶Œìž¥)
        print(f"Optimization Error with target {target_ratio}: {e}", flush=True)
        w = w0
    
    w = clean_weights_array(w)

    port_var = float(w @ cov @ w)
    if port_var < 0:
        port_var = 0.0
    vol = float(np.sqrt(port_var))
    ret = float(w @ mu)

    return w, vol, ret


def compute_efficient_frontier(df, n_points=40):
    """
    íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê³„ì‚°
    df: ìˆ˜ìµë¥  DataFrame
    n_points: í”„ë¡ í‹°ì–´ ìƒ˜í”Œ ê°œìˆ˜
    """
    import numpy as np

    print("\n=== [EF] START efficient frontier ===", flush=True)

    if df.empty:
        print("[EF] ERROR: df is empty!", flush=True)
        return [], [], []

    risks = []
    returns = []
    weights_list = []

    grid = np.linspace(0.0, 1.0, n_points)
    print("[EF] grid:", grid, flush=True)

    for tr in grid:
        print(f"\n[EF] ---- target={tr:.3f} ----", flush=True)
        try:
            w, r, ret = compute_target_risk(df, tr)
            print(f"[EF] w: {w}", flush=True)
            print(f"[EF] risk={r}, ret={ret}", flush=True)

            if np.isfinite(r) and np.isfinite(ret):
                risks.append(float(r))
                returns.append(float(ret))
                weights_list.append(w.tolist())
            else:
                print("[EF] nan detected, skip", flush=True)

        except Exception as e:
            print(f"[EF] ERROR at target {tr}: {e}", flush=True)
            continue

    print("=== [EF] END efficient frontier ===\n", flush=True)
    return risks, returns, weights_list

def compute_frontier_markers(df: pd.DataFrame, annual_freq: int = 252):
    """
    íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê·¸ëž˜í”„ì— í‘œì‹œí•  4ê°€ì§€ í•µì‹¬ í¬íŠ¸í´ë¦¬ì˜¤ ì¢Œí‘œ ê³„ì‚°
    1. GMV (Global Minimum Variance): ìµœì†Œ ë¶„ì‚°
    2. MST (Max Sharpe Tangency): ìµœëŒ€ ìƒ¤í”„ ì§€ìˆ˜
    3. RP (Risk Parity): ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹°
    4. MDP (Max Diversification): ìµœëŒ€ ë¶„ì‚° íš¨ê³¼
    """
    from pypfopt.efficient_frontier import EfficientFrontier
    from pypfopt import risk_models, expected_returns
    
    results = {}
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df = _clean_returns_df(df)
    if df.empty:
        return {}

    # ê¸°ëŒ€ìˆ˜ìµë¥ (mu)ê³¼ ê³µë¶„ì‚°í–‰ë ¬(S) ê³„ì‚°
    mu = expected_returns.mean_historical_return(df, returns_data=True, compounding=True, frequency=annual_freq)
    S = risk_models.sample_cov(df, returns_data=True, frequency=annual_freq)

    # --- 1. ìµœì†Œ ë¶„ì‚° (Min Volatility) ---
    try:
        ef = EfficientFrontier(mu, S)
        ef.min_volatility()
        w_gmv = clean_weights_array(_weights_dict_to_array(ef.clean_weights(), df.columns))
        r_gmv, ret_gmv = _compute_portfolio_stats(df, w_gmv, annual_freq)
        results['min_volatility'] = {"risk": r_gmv, "return": ret_gmv, "weights": w_gmv.tolist()}
    except Exception as e:
        print(f"MinVol Error: {e}")
        results['min_volatility'] = None

    # --- 2. ìµœëŒ€ ìƒ¤í”„ (Max Sharpe) ---
    try:
        # risk_free_rateëŠ” 0.02(2%) í˜¹ì€ 0ìœ¼ë¡œ ê°€ì •
        ef = EfficientFrontier(mu, S)
        ef.max_sharpe(risk_free_rate=0.0) 
        w_mst = clean_weights_array(_weights_dict_to_array(ef.clean_weights(), df.columns))
        r_mst, ret_mst = _compute_portfolio_stats(df, w_mst, annual_freq)
        results['max_sharpe'] = {"risk": r_mst, "return": ret_mst, "weights": w_mst.tolist()}
    except Exception as e:
        print(f"MaxSharpe Error: {e}")
        results['max_sharpe'] = None

    # --- 3. ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹° (Risk Parity) ---
    try:
        w_rp, r_rp, ret_rp = compute_risk_parity(df, annual_freq)
        results['risk_parity'] = {"risk": r_rp, "return": ret_rp, "weights": w_rp.tolist()}
    except Exception as e:
        print(f"RP Error: {e}")
        results['risk_parity'] = None

    # --- 4. ìµœëŒ€ ë¶„ì‚° ë¹„ìœ¨ (Max Diversification) ---
    try:
        # MDP ëª©ì í•¨ìˆ˜: - (ê°€ì¤‘í‰ê·  ê°œë³„ ë³€ë™ì„± / í¬íŠ¸í´ë¦¬ì˜¤ ë³€ë™ì„±)
        vol = np.sqrt(np.diag(S)) # ê°œë³„ ìžì‚° ë³€ë™ì„±
        
        def calc_div_ratio(w):
            w = np.array(w)
            w_vol = np.dot(vol, w) # ë¶„ìž: ê°œë³„ ë¦¬ìŠ¤í¬ì˜ í•©
            port_vol = np.sqrt(w.T @ S @ w) # ë¶„ëª¨: í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬
            return -(w_vol / port_vol)

        cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
        bounds = tuple((0, 1) for _ in range(len(df.columns)))
        w0 = np.array([1/len(df.columns)] * len(df.columns))
        
        res = minimize(calc_div_ratio, w0, method='SLSQP', bounds=bounds, constraints=cons)
        w_mdp = clean_weights_array(res.x)
        r_mdp, ret_mdp = _compute_portfolio_stats(df, w_mdp, annual_freq)
        
        results['max_diversification'] = {"risk": r_mdp, "return": ret_mdp, "weights": w_mdp.tolist()}
    except Exception as e:
        print(f"MDP Error: {e}")
        results['max_diversification'] = None

    return results
